{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 KNN 类\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', weighted=False):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        # 计算类别权重\n",
    "        if self.weighted:\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            total = len(y)\n",
    "            self.class_weights = {u: total / (len(unique) * c) for u, c in zip(unique, counts)}\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_distance(self, X):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # 高效的向量化欧氏距离计算\n",
    "            X_squared = np.sum(X**2, axis=1).reshape(-1, 1).astype(np.float32)\n",
    "            train_squared = np.sum(self.X_train**2, axis=1).reshape(1, -1).astype(np.float32)\n",
    "            cross_term = np.dot(X, self.X_train.T).astype(np.float32)\n",
    "            distances = np.sqrt(X_squared + train_squared - 2 * cross_term)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            # 向量化曼哈顿距离计算，分批处理以节省内存\n",
    "            n_test = X.shape[0]\n",
    "            n_train = self.X_train.shape[0]\n",
    "            distances = np.zeros((n_test, n_train), dtype=np.float32)\n",
    "            batch_size = 500  # 根据内存情况调整\n",
    "            for i in range(0, n_test, batch_size):\n",
    "                end = min(i + batch_size, n_test)\n",
    "                batch = X[i:end, :]  # (batch_size, n_features)\n",
    "                # 计算绝对差值并求和\n",
    "                distances[i:end, :] = np.sum(np.abs(batch[:, np.newaxis, :] - self.X_train), axis=2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "        return distances\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        distances = self.compute_distance(X)  # 形状 (n_test, n_train)\n",
    "        neighbor_indices = np.argsort(distances, axis=1)[:, :self.k]  # 形状 (n_test, k)\n",
    "        neighbor_labels = self.y_train[neighbor_indices]  # 形状 (n_test, k)\n",
    "        if self.weighted:\n",
    "            neighbor_distances = np.take_along_axis(distances, neighbor_indices, axis=1)  # 形状 (n_test, k)\n",
    "            weights = 1 / (neighbor_distances + 1e-5)  # 避免除以零\n",
    "            class_weights = np.vectorize(self.class_weights.get)(neighbor_labels)  # 形状 (n_test, k)\n",
    "            weighted_labels = neighbor_labels * class_weights  # 形状 (n_test, k)\n",
    "            proba = np.sum(weights * weighted_labels, axis=1) / np.sum(weights * class_weights, axis=1)\n",
    "        else:\n",
    "            if self.class_weights:\n",
    "                class_weights = np.vectorize(self.class_weights.get)(neighbor_labels)  # 形状 (n_test, k)\n",
    "                weighted_labels = neighbor_labels * class_weights\n",
    "                proba = np.mean(weighted_labels, axis=1)\n",
    "            else:\n",
    "                proba = np.mean(neighbor_labels, axis=1)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "# 定义集成 KNN 类\n",
    "class EnsembleKNN:\n",
    "    def __init__(self, knn_models):\n",
    "        self.knn_models = knn_models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for knn in self.knn_models:\n",
    "            knn.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = np.mean([knn.predict_proba(X) for knn in self.knn_models], axis=0)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据预处理函数\n",
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # 合并训练和测试数据以确保一致的预处理\n",
    "    combined = pd.concat([train_data.drop('Exited', axis=1), test_data], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # 删除不必要的列\n",
    "    combined = combined.drop(['CustomerId', 'Surname', 'id'], axis=1)\n",
    "\n",
    "    # 特征工程：创建交互特征\n",
    "    combined['Age_Tenure_Ratio'] = combined['Age'] / (combined['Tenure'] + 1)\n",
    "    combined['Balance_EstimatedSalary_Ratio'] = combined['Balance'] / (combined['EstimatedSalary'] + 1)\n",
    "    combined['NumOfProducts_IsActiveMember'] = combined['NumOfProducts'] * combined['IsActiveMember']\n",
    "    combined['Age_Balance_Ratio'] = combined['Age'] / (combined['Balance'] + 1)\n",
    "    combined['CreditScore_Age'] = combined['CreditScore'] * combined['Age']\n",
    "    combined['CreditScore_Balance'] = combined['CreditScore'] * combined['Balance']\n",
    "\n",
    "    # 转换偏态分布的特征\n",
    "    for col in ['Balance', 'EstimatedSalary']:\n",
    "        combined[col] = combined[col].apply(lambda x: np.log1p(x) if x > 0 else 0)\n",
    "\n",
    "    # 独热编码分类变量\n",
    "    combined = pd.get_dummies(combined, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "    # 特征缩放使用 RobustScaler 以减少异常值的影响\n",
    "    scaler = RobustScaler()\n",
    "    scaled_features = scaler.fit_transform(combined)\n",
    "\n",
    "    # 分割回训练和测试数据\n",
    "    X_train = scaled_features[:train_data.shape[0], :]\n",
    "    y_train = train_data['Exited'].values\n",
    "    X_test = scaled_features[train_data.shape[0]:, :]\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义并行化的交叉验证函数\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    auc_scores = Parallel(n_jobs=-1)(delayed(evaluate_fold)(X, y, knn, train_idx, val_idx)\n",
    "                                     for train_idx, val_idx in skf.split(X, y))\n",
    "    return auc_scores\n",
    "\n",
    "def evaluate_fold(X, y, knn, train_idx, val_idx):\n",
    "    X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "    y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "\n",
    "    knn.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_proba = knn.predict_proba(X_val_cv)\n",
    "\n",
    "    auc = roc_auc_score(y_val_cv, y_pred_proba)\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3, metric=euclidean, weighted=False, AUC=0.8421\n",
      "k=3, metric=euclidean, weighted=True, AUC=0.8435\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.56 GiB for an array with shape (3000, 12000, 17) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\python\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\python\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\python\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\python\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34680\\291167758.py\", line 13, in evaluate_fold\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34680\\2652507072.py\", line 34, in predict_proba\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_34680\\2652507072.py\", line 28, in compute_distance\nnumpy._core._exceptions._ArrayMemoryError: Unable to allocate 4.56 GiB for an array with shape (3000, 12000, 17) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weighted \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]:\n\u001b[0;32m     14\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNN(k\u001b[38;5;241m=\u001b[39mk, distance_metric\u001b[38;5;241m=\u001b[39mmetric, weighted\u001b[38;5;241m=\u001b[39mweighted)\n\u001b[1;32m---> 15\u001b[0m     cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     avg_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cv_scores)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, metric=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, weighted=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweighted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[99], line 4\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(X, y, knn, n_splits)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_validate\u001b[39m(X, y, knn, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      3\u001b[0m     skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mn_splits, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     auc_scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc_scores\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.56 GiB for an array with shape (3000, 12000, 17) and data type float64"
     ]
    }
   ],
   "source": [
    "# 加载和预处理数据\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# 超参数调优\n",
    "best_k = 9\n",
    "best_metric = 'manhattan'\n",
    "best_weighted = True\n",
    "best_score = 0\n",
    "\n",
    "# 探索更广泛的 k 值和不同的距离度量\n",
    "for k in [3, 5, 7, 9, 11, 13, 15]:\n",
    "    for metric in ['euclidean', 'manhattan']:\n",
    "        for weighted in [False, True]:\n",
    "            knn = KNN(k=k, distance_metric=metric, weighted=weighted)\n",
    "            cv_scores = cross_validate(X, y, knn)\n",
    "            avg_score = np.mean(cv_scores)\n",
    "            print(f\"k={k}, metric={metric}, weighted={weighted}, AUC={avg_score:.4f}\")\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_k = k\n",
    "                best_metric = metric\n",
    "                best_weighted = weighted\n",
    "\n",
    "print(f\"Best hyperparameters: k={best_k}, metric={best_metric}, weighted={best_weighted}, AUC={best_score:.4f}\")\n",
    "\n",
    "# 使用最佳超参数创建 KNN 模型并训练\n",
    "best_knn = KNN(k=best_k, distance_metric=best_metric, weighted=best_weighted)\n",
    "best_knn.fit(X, y)\n",
    "test_predictions_proba = best_knn.predict_proba(X_test)\n",
    "\n",
    "# 如果你选择使用集成模型，可以如下操作：\n",
    "# knn1 = KNN(k=5, distance_metric='manhattan', weighted=True)\n",
    "# knn2 = KNN(k=7, distance_metric='manhattan', weighted=True)\n",
    "# knn3 = KNN(k=9, distance_metric='manhattan', weighted=True)\n",
    "# ensemble_knn = EnsembleKNN([knn1, knn2, knn3])\n",
    "# ensemble_knn.fit(X, y)\n",
    "# test_predictions_proba = ensemble_knn.predict_proba(X_test)\n",
    "\n",
    "# 生成提交文件\n",
    "submission = pd.read_csv('test.csv')[['id']]\n",
    "submission['Exited'] = test_predictions_proba\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
